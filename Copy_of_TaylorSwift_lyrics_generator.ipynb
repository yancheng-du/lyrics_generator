{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TaylorSwift_lyrics_generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yancheng-du/lyrics_generator/blob/main/Copy_of_TaylorSwift_lyrics_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r5jgH2FZbGB"
      },
      "source": [
        "# **Taylor Swift Lyrics Generator**\n",
        "<!-- ![](https://cdn-images-1.medium.com/max/1800/1*VSXmKKJJZFlUVVZWBwbXVg.jpeg) -->\n",
        "\n",
        "A few days ago, I started to learn LSTM RNN (Long Short Term Memory Recurrent Neural Networks), and I thought that it would be a good idea if I make a project using it.\n",
        "\n",
        "There is a multitude of applications of LSTM RNN, I decided to go with natural language generation because it will be a good opportunity to learn how to process text data, and it will be entertaining to see texts generated by neural networks, so I got this idea about generating Taylor Swift lyrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5iAhbcOdsrX"
      },
      "source": [
        "## **What is LSTM Recurrent Neural Networks ?**\n",
        "If you don't know, LSTM recurrent neural networks are networks with loops in them, allowing information to persist, and they have a special type of nodes called LSTM(Long Short Term Memory).\n",
        "\n",
        "LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.\n",
        "If you want to know more about  LSTM Recurrent Neural Networks visit :\n",
        "[Understanding LSTM Networks](https://goo.gl/dgmxQm) or [Long short-term memoryt](https://goo.gl/Dc7kHF)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7KxCwI0ewIN"
      },
      "source": [
        "## LSTM Recurrent Neural Networks Applications \n",
        "LSTM Recurrent Neural Networks are used in many applicattions , the following are the most popular ones :\n",
        "\n",
        "\n",
        "\n",
        "*   Language modeling\n",
        "*   Text classification\n",
        "*   Dialog systems\n",
        "*   **Natural language generation**\n",
        "\n",
        "[More applications](https://goo.gl/eT3bMm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiAiWbbKjx2v"
      },
      "source": [
        "Now, after we learned some essential information about LSTM and RNN , we will start implementing the idea (Taylor Swift Lyrics Generator) \n",
        "\n",
        "I will use two ways to build the model :\n",
        "* From scratch\n",
        "* Using a Python module called [textgenrnn](https://goo.gl/E7szXj)\n",
        "\n",
        "## Process The Dataset \n",
        "To train the LSTM model we need a dataset of Taylor songs' lyrics.\n",
        "After searching for it, I found [this great dataset](https://goo.gl/3oUpMG) in Kaggle .\n",
        "\n",
        "**Let's take a look at it :**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_K2b3bUl824"
      },
      "source": [
        "first, import all the needed libraries for our project:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzrnYELquoXL",
        "outputId": "70d1d3dd-8f91-48e9-e875-3191d6001ec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "pip install --upgrade keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3_CZ9W6vFzN"
      },
      "source": [
        "from keras.layers import LSTM\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coOsymS4l3Cq"
      },
      "source": [
        "# Import the dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys \n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Activation, Flatten, Dropout, Dense, Embedding, TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d87T5nx4mlYP"
      },
      "source": [
        "**Load the dataset :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn11dO596zzH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlevTCJ4mjj2"
      },
      "source": [
        "#Load the dataset\n",
        "url = 'https://raw.githubusercontent.com/yancheng-du/lyrics_generator/main/taylor_swift_lyrics.csv'\n",
        "dataset = pd.read_csv(url, encoding = \"latin1\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF7rXKwdnAd9",
        "outputId": "5f68d66a-bd06-44bf-9834-5be2992d2750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>artist</th>\n",
              "      <th>album</th>\n",
              "      <th>track_title</th>\n",
              "      <th>track_n</th>\n",
              "      <th>lyric</th>\n",
              "      <th>line</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>He said the way my blue eyes shined</td>\n",
              "      <td>1</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>Put those Georgia stars to shame that night</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>I said, \"That's a lie\"</td>\n",
              "      <td>3</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>Just a boy in a Chevy truck</td>\n",
              "      <td>4</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Taylor Swift</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>1</td>\n",
              "      <td>That had a tendency of gettin' stuck</td>\n",
              "      <td>5</td>\n",
              "      <td>2006</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         artist         album  ... line  year\n",
              "0  Taylor Swift  Taylor Swift  ...    1  2006\n",
              "1  Taylor Swift  Taylor Swift  ...    2  2006\n",
              "2  Taylor Swift  Taylor Swift  ...    3  2006\n",
              "3  Taylor Swift  Taylor Swift  ...    4  2006\n",
              "4  Taylor Swift  Taylor Swift  ...    5  2006\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET89hoHGnHz9",
        "outputId": "3bdb7fc1-7ff7-47ac-c0e5-ace54e7277c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "dataset.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>track_n</th>\n",
              "      <th>line</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4862.000000</td>\n",
              "      <td>4862.000000</td>\n",
              "      <td>4862.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>8.216989</td>\n",
              "      <td>28.426573</td>\n",
              "      <td>2011.882764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.696379</td>\n",
              "      <td>18.343649</td>\n",
              "      <td>3.571447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2006.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>4.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>2010.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>8.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>2012.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>12.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>2014.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>19.000000</td>\n",
              "      <td>101.000000</td>\n",
              "      <td>2017.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           track_n         line         year\n",
              "count  4862.000000  4862.000000  4862.000000\n",
              "mean      8.216989    28.426573  2011.882764\n",
              "std       4.696379    18.343649     3.571447\n",
              "min       1.000000     1.000000  2006.000000\n",
              "25%       4.000000    13.000000  2010.000000\n",
              "50%       8.000000    26.000000  2012.000000\n",
              "75%      12.000000    41.000000  2014.000000\n",
              "max      19.000000   101.000000  2017.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeN5LhosnYD2"
      },
      "source": [
        "From what we see , we need to concatenate the lines of each song to get each song by its own in one string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DFbVyycoFek"
      },
      "source": [
        "def processFirstLine(lyrics, songID, songName, row):\n",
        "    lyrics.append(row['lyric'] + '\\n')\n",
        "    songID.append( row['year']*100+ row['track_n'])\n",
        "    songName.append(row['track_title'])\n",
        "    return lyrics,songID,songName"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jTFUaamn19J"
      },
      "source": [
        "# define empty lists for the lyrics , songID , songName \n",
        "lyrics = []\n",
        "songID = []\n",
        "songName = []\n",
        "\n",
        "# songNumber indicates the song number in the dataset\n",
        "songNumber = 1\n",
        "\n",
        "# i indicates the song number\n",
        "i = 0\n",
        "isFirstLine = True\n",
        "\n",
        "# Iterate through every lyrics line and join them together for each song independently \n",
        "for index,row in dataset.iterrows():\n",
        "    if(songNumber == row['track_n']):\n",
        "        if (isFirstLine):\n",
        "            lyrics,songID,songName = processFirstLine(lyrics,songID,songName,row)\n",
        "            isFirstLine = False\n",
        "        else :\n",
        "            #if we still in the same song , keep joining the lyrics lines    \n",
        "            lyrics[i] +=  row['lyric'] + '\\n'\n",
        "    #When it's done joining a song's lyrics lines , go to the next song :    \n",
        "    else :\n",
        "        lyrics,songID,songName = processFirstLine(lyrics,songID,songName,row)\n",
        "        songNumber = row['track_n']\n",
        "        i+=1\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lEEwMvhpZ_2",
        "outputId": "d77d1ed7-8b8f-4c1c-b291-b7dc5e3ff756",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "\n",
        "# Define a new pandas DataFrame to save songID , songName , Lyrics in it to use them later\n",
        "lyrics_data = pd.DataFrame({'songID':songID, 'songName':songName, 'lyrics':lyrics })\n",
        "\n",
        "lyrics_data[:5]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>songID</th>\n",
              "      <th>songName</th>\n",
              "      <th>lyrics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>200601</td>\n",
              "      <td>Tim McGraw</td>\n",
              "      <td>He said the way my blue eyes shined\\nPut those...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>200602</td>\n",
              "      <td>Picture To Burn</td>\n",
              "      <td>State the obvious, I didn't get my perfect fan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>200603</td>\n",
              "      <td>Teardrops On My Guitar</td>\n",
              "      <td>Drew looks at me\\nI fake a smile so he won't s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>200604</td>\n",
              "      <td>A Place In This World</td>\n",
              "      <td>I don't know what I want, so don't ask me\\nCau...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>200605</td>\n",
              "      <td>Cold as You</td>\n",
              "      <td>You have a way of coming easily to me\\nAnd whe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   songID  ...                                             lyrics\n",
              "0  200601  ...  He said the way my blue eyes shined\\nPut those...\n",
              "1  200602  ...  State the obvious, I didn't get my perfect fan...\n",
              "2  200603  ...  Drew looks at me\\nI fake a smile so he won't s...\n",
              "3  200604  ...  I don't know what I want, so don't ask me\\nCau...\n",
              "4  200605  ...  You have a way of coming easily to me\\nAnd whe...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4CJdW-2pvHU"
      },
      "source": [
        "Now save the lyrics in a text file to use it in the LSTM RNN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N48sFRVHqFpq"
      },
      "source": [
        "# Save Lyrics in .txt file\n",
        "with open('lyricsText.txt', 'w',encoding=\"utf-8\") as filehandle:  \n",
        "    for listitem in lyrics:\n",
        "        filehandle.write('%s\\n' % listitem)\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29Dii_ZiqYlf"
      },
      "source": [
        "After getting the wanted data from the dataset , we need to preprocess it.\n",
        "\n",
        "## Preprocessing The Lyrics\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBR3GAjDQ_nL"
      },
      "source": [
        "### 1- Convert the lyrics to lowercase :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVOSN8qIqz0P"
      },
      "source": [
        "# Load the dataset and convert it to lowercase :\n",
        "textFileName = 'lyricsText.txt'\n",
        "raw_text = open(textFileName, encoding = 'UTF-8').read()\n",
        "raw_text = raw_text.lower()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4wKS9n5rpNg"
      },
      "source": [
        "### 2- Mapping characters :\n",
        " Make two dictionaries , one to convert chars to ints , the other to convert ints back to chars : \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jafC0WcvsA4R"
      },
      "source": [
        "# Mapping chars to ints :\n",
        "chars = sorted(list(set(raw_text)))\n",
        "int_chars = dict((i, c) for i, c in enumerate(chars))\n",
        "chars_int = dict((i, c) for c, i in enumerate(chars))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsWUvqD0sGsX"
      },
      "source": [
        "# Get number of chars and vocab in our text :\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSItHMeqsMyR",
        "outputId": "925ad69f-6972-4e80-db58-6b968f67730c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print('Total Characters : ' , n_chars) # number of all the characters in lyricsText.txt\n",
        "print('Total Vocab : ', n_vocab) # number of unique characters\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters :  173698\n",
            "Total Vocab :  58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErC8jLlDsxpH"
      },
      "source": [
        "### 3- Make samples and labels :\n",
        "Make samples and labels to feed the LSTM RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7vPB3K6s-6v",
        "outputId": "8c6fee87-9a4d-4f08-a573-9fd656867c28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# process the dataset:\n",
        "seq_len = 100\n",
        "data_X = []\n",
        "data_y = []\n",
        "\n",
        "for i in range(0, n_chars - seq_len, 1):\n",
        "    # Input Sequeance(will be used as samples)\n",
        "    seq_in  = raw_text[i:i+seq_len]\n",
        "    # Output sequence (will be used as target)\n",
        "    seq_out = raw_text[i + seq_len]\n",
        "    # Store samples in data_X\n",
        "    data_X.append([chars_int[char] for char in seq_in])\n",
        "    # Store targets in data_y\n",
        "    data_y.append(chars_int[seq_out])\n",
        "n_patterns = len(data_X)\n",
        "print( 'Total Patterns : ', n_patterns)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns :  173598\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHF-E1QLuAf6"
      },
      "source": [
        "### 4- Prepare the samples and labels :\n",
        "prepare the samples and labels to be ready to go into our model.\n",
        "* Reshape the samples\n",
        "* Normalize them\n",
        "* One hot encode the output targets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HL0q0DouKei"
      },
      "source": [
        "# Reshape X to be suitable to go into LSTM RNN :\n",
        "X = np.reshape(data_X , (n_patterns, seq_len, 1))\n",
        "# Normalizing input data :\n",
        "X = X/ float(n_vocab)\n",
        "# One hot encode the output targets :\n",
        "y = np_utils.to_categorical(data_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB9FHOuWzSMz"
      },
      "source": [
        "After we finished processing the dataset , we will start building our LSTM RNN model .\n",
        "\n",
        "## Building The Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwg_rARt92L7"
      },
      "source": [
        "## First way : From Scratch "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbVsgyoj0R9G"
      },
      "source": [
        "We will start by determining how many layers our model will has , and how many nodes each layer will has :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEBOjT8b0puT"
      },
      "source": [
        "LSTM_layer_num = 4 # number of LSTM layers\n",
        "layer_size = [256,256,256,256] # number of nodes in each layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaZgebyX0_DK"
      },
      "source": [
        "Define a sequential model :\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VoVZqCY1GlC"
      },
      "source": [
        "model = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1qmHybP50Mf"
      },
      "source": [
        "### LSTM layer VS CuDNNLSTM layer \n",
        "The main difference is that LSTM uses the CPU and CuDNNLSTM uses the GPU , that's why CuDNNLSTM is much faster than LSTM , it is x15 faster.\n",
        "\n",
        "This is the reason that made me use CuDNNLTSM instead of LSTM .\n",
        "\n",
        "**Note :** make sure to change the runtime setting of colab to use its GPU ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5q0sEfv1IJg"
      },
      "source": [
        "Add an input layer :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBCy2aCI1NEn"
      },
      "source": [
        "model.add(CuDNNLSTM(layer_size[0], input_shape =(X.shape[1], X.shape[2]), return_sequences = True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbQE1EYX1TLD"
      },
      "source": [
        "Add some hidden layers : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CngOC8xX1YcL"
      },
      "source": [
        "for i in range(1,LSTM_layer_num) :\n",
        "    model.add(CuDNNLSTM(layer_size[i], return_sequences=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc7Rd3WE1hTJ"
      },
      "source": [
        "Flatten the data that is coming from the last hidden layer to input it to the output layer :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZvmlrtO1tkO"
      },
      "source": [
        "model.add(Flatten())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWlyuZM91yBt"
      },
      "source": [
        "Add an output layer and define its activation function to be **'softmax'** \n",
        "\n",
        "and then compile the model with the next params :\n",
        "*  loss = 'categorical_crossentropy'\n",
        "*  optimizer = 'adam'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuaTsnQa1wSY"
      },
      "source": [
        "model.add(Dense(y.shape[1]))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bBWrlrV5EA1"
      },
      "source": [
        "Print a summary of the model to see some details :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0_EwdN65L94",
        "outputId": "7f8c79c5-a634-4eb9-c7bb-98ec03190b65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "cu_dnnlstm_1 (CuDNNLSTM)     (None, 100, 256)          265216    \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_2 (CuDNNLSTM)     (None, 100, 256)          526336    \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_3 (CuDNNLSTM)     (None, 100, 256)          526336    \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_4 (CuDNNLSTM)     (None, 100, 256)          526336    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25600)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 58)                1484858   \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 58)                0         \n",
            "=================================================================\n",
            "Total params: 3,329,082\n",
            "Trainable params: 3,329,082\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKtPQlVW2fwm"
      },
      "source": [
        "After we defined the model , we will define the needed callbacks.\n",
        "\n",
        "### What is a callback ?\n",
        "A callback is a function that is called after every epoch\n",
        "\n",
        "in our case we will call the checkpoint callback , what a checkpoint callback does is saving the weights of the model every time the model gets better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9agmjvU3gNL"
      },
      "source": [
        "# Configure the checkpoint :\n",
        "checkpoint_name = 'Weights-LSTM-improvement-{epoch:03d}-{loss:.5f}-bigger.hdf5'\n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='loss', verbose = 1, save_best_only = True, mode ='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G3g4N7A3iTo"
      },
      "source": [
        "## Training \n",
        "A model can't do a thing if it did not train.\n",
        "\n",
        "As they say **\"No train no gain \"**\n",
        "\n",
        "Feel free to tweak `model_params` to get a better model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a95hUKQf4OnJ",
        "outputId": "7b7feb5f-f5ed-4e1e-d43c-24aabfa18054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2255
        }
      },
      "source": [
        "# Fit the model :\n",
        "model_params = {'epochs':30,\n",
        "                'batch_size':128,\n",
        "                'callbacks':callbacks_list,\n",
        "                'verbose':1,\n",
        "                'validation_split':0.2,\n",
        "                'validation_data':None,\n",
        "                'shuffle': True,\n",
        "                'initial_epoch':0,\n",
        "                'steps_per_epoch':None,\n",
        "                'validation_steps':None}\n",
        "\n",
        "model.fit(X,\n",
        "          y,\n",
        "          epochs = model_params['epochs'],\n",
        "           batch_size = model_params['batch_size'],\n",
        "           callbacks= model_params['callbacks'],\n",
        "           verbose = model_params['verbose'],\n",
        "           validation_split = model_params['validation_split'],\n",
        "           validation_data = model_params['validation_data'],\n",
        "           shuffle = model_params['shuffle'],\n",
        "           initial_epoch = model_params['initial_epoch'],\n",
        "           steps_per_epoch = model_params['steps_per_epoch'],\n",
        "           validation_steps = model_params['validation_steps'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 138878 samples, validate on 34720 samples\n",
            "Epoch 1/30\n",
            "138878/138878 [==============================] - 186s 1ms/step - loss: 2.8300 - val_loss: 2.7864\n",
            "\n",
            "Epoch 00001: loss improved from 3.00537 to 2.82996, saving model to Weights-LSTM-improvement-001-2.82996-bigger.hdf5\n",
            "Epoch 2/30\n",
            "138878/138878 [==============================] - 186s 1ms/step - loss: 2.6424 - val_loss: 2.6723\n",
            "\n",
            "Epoch 00002: loss improved from 2.82996 to 2.64236, saving model to Weights-LSTM-improvement-002-2.64236-bigger.hdf5\n",
            "Epoch 3/30\n",
            "138878/138878 [==============================] - 186s 1ms/step - loss: 2.3721 - val_loss: 2.5978\n",
            "\n",
            "Epoch 00003: loss improved from 2.64236 to 2.37208, saving model to Weights-LSTM-improvement-003-2.37208-bigger.hdf5\n",
            "Epoch 4/30\n",
            "138878/138878 [==============================] - 186s 1ms/step - loss: 1.9650 - val_loss: 2.6343\n",
            "\n",
            "Epoch 00004: loss improved from 2.37208 to 1.96500, saving model to Weights-LSTM-improvement-004-1.96500-bigger.hdf5\n",
            "Epoch 5/30\n",
            "138878/138878 [==============================] - 186s 1ms/step - loss: 1.5298 - val_loss: 2.6932\n",
            "\n",
            "Epoch 00005: loss improved from 1.96500 to 1.52981, saving model to Weights-LSTM-improvement-005-1.52981-bigger.hdf5\n",
            "Epoch 6/30\n",
            "138878/138878 [==============================] - 186s 1ms/step - loss: 1.1555 - val_loss: 2.8772\n",
            "\n",
            "Epoch 00006: loss improved from 1.52981 to 1.15551, saving model to Weights-LSTM-improvement-006-1.15551-bigger.hdf5\n",
            "Epoch 7/30\n",
            "138878/138878 [==============================] - 186s 1ms/step - loss: 0.8443 - val_loss: 3.1771\n",
            "\n",
            "Epoch 00007: loss improved from 1.15551 to 0.84430, saving model to Weights-LSTM-improvement-007-0.84430-bigger.hdf5\n",
            "Epoch 8/30\n",
            "138878/138878 [==============================] - 187s 1ms/step - loss: 0.5888 - val_loss: 3.4821\n",
            "\n",
            "Epoch 00008: loss improved from 0.84430 to 0.58876, saving model to Weights-LSTM-improvement-008-0.58876-bigger.hdf5\n",
            "Epoch 9/30\n",
            "138878/138878 [==============================] - 187s 1ms/step - loss: 0.3961 - val_loss: 3.9257\n",
            "\n",
            "Epoch 00009: loss improved from 0.58876 to 0.39615, saving model to Weights-LSTM-improvement-009-0.39615-bigger.hdf5\n",
            "Epoch 10/30\n",
            "138878/138878 [==============================] - 187s 1ms/step - loss: 0.2646 - val_loss: 4.2018\n",
            "\n",
            "Epoch 00010: loss improved from 0.39615 to 0.26464, saving model to Weights-LSTM-improvement-010-0.26464-bigger.hdf5\n",
            "Epoch 11/30\n",
            "138878/138878 [==============================] - 186s 1ms/step - loss: 0.1873 - val_loss: 4.5327\n",
            "\n",
            "Epoch 00011: loss improved from 0.26464 to 0.18727, saving model to Weights-LSTM-improvement-011-0.18727-bigger.hdf5\n",
            "Epoch 12/30\n",
            "138878/138878 [==============================] - 187s 1ms/step - loss: 0.1476 - val_loss: 4.7776\n",
            "\n",
            "Epoch 00012: loss improved from 0.18727 to 0.14758, saving model to Weights-LSTM-improvement-012-0.14758-bigger.hdf5\n",
            "Epoch 13/30\n",
            "138878/138878 [==============================] - 186s 1ms/step - loss: 0.1292 - val_loss: 4.9543\n",
            "\n",
            "Epoch 00013: loss improved from 0.14758 to 0.12921, saving model to Weights-LSTM-improvement-013-0.12921-bigger.hdf5\n",
            "Epoch 14/30\n",
            "138878/138878 [==============================] - 187s 1ms/step - loss: 0.1188 - val_loss: 5.1165\n",
            "\n",
            "Epoch 00014: loss improved from 0.12921 to 0.11878, saving model to Weights-LSTM-improvement-014-0.11878-bigger.hdf5\n",
            "Epoch 15/30\n",
            "138878/138878 [==============================] - 187s 1ms/step - loss: 0.1109 - val_loss: 5.1407\n",
            "\n",
            "Epoch 00015: loss improved from 0.11878 to 0.11092, saving model to Weights-LSTM-improvement-015-0.11092-bigger.hdf5\n",
            "Epoch 16/30\n",
            "138878/138878 [==============================] - 187s 1ms/step - loss: 0.0986 - val_loss: 5.3270\n",
            "\n",
            "Epoch 00016: loss improved from 0.11092 to 0.09858, saving model to Weights-LSTM-improvement-016-0.09858-bigger.hdf5\n",
            "Epoch 17/30\n",
            "138878/138878 [==============================] - 187s 1ms/step - loss: 0.0949 - val_loss: 5.5357\n",
            "\n",
            "Epoch 00017: loss improved from 0.09858 to 0.09487, saving model to Weights-LSTM-improvement-017-0.09487-bigger.hdf5\n",
            "Epoch 18/30\n",
            "138878/138878 [==============================] - 186s 1ms/step - loss: 0.0963 - val_loss: 5.4257\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.09487\n",
            "Epoch 19/30\n",
            "138878/138878 [==============================] - 186s 1ms/step - loss: 0.0906 - val_loss: 5.5971\n",
            "\n",
            "Epoch 00019: loss improved from 0.09487 to 0.09062, saving model to Weights-LSTM-improvement-019-0.09062-bigger.hdf5\n",
            "Epoch 20/30\n",
            "138878/138878 [==============================] - 187s 1ms/step - loss: 0.0862 - val_loss: 5.8942\n",
            "\n",
            "Epoch 00020: loss improved from 0.09062 to 0.08617, saving model to Weights-LSTM-improvement-020-0.08617-bigger.hdf5\n",
            "Epoch 21/30\n",
            "138878/138878 [==============================] - 186s 1ms/step - loss: 0.0828 - val_loss: 5.8484\n",
            "\n",
            "Epoch 00021: loss improved from 0.08617 to 0.08280, saving model to Weights-LSTM-improvement-021-0.08280-bigger.hdf5\n",
            "Epoch 22/30\n",
            "138878/138878 [==============================] - 186s 1ms/step - loss: 0.0784 - val_loss: 5.8140\n",
            "\n",
            "Epoch 00022: loss improved from 0.08280 to 0.07837, saving model to Weights-LSTM-improvement-022-0.07837-bigger.hdf5\n",
            "Epoch 23/30\n",
            "138878/138878 [==============================] - 187s 1ms/step - loss: 0.0779 - val_loss: 5.9245\n",
            "\n",
            "Epoch 00023: loss improved from 0.07837 to 0.07792, saving model to Weights-LSTM-improvement-023-0.07792-bigger.hdf5\n",
            "Epoch 24/30\n",
            "138878/138878 [==============================] - 187s 1ms/step - loss: 0.0767 - val_loss: 6.0071\n",
            "\n",
            "Epoch 00024: loss improved from 0.07792 to 0.07667, saving model to Weights-LSTM-improvement-024-0.07667-bigger.hdf5\n",
            "Epoch 25/30\n",
            "138878/138878 [==============================] - 187s 1ms/step - loss: 0.0763 - val_loss: 6.0937\n",
            "\n",
            "Epoch 00025: loss improved from 0.07667 to 0.07627, saving model to Weights-LSTM-improvement-025-0.07627-bigger.hdf5\n",
            "Epoch 26/30\n",
            "138878/138878 [==============================] - 187s 1ms/step - loss: 0.0759 - val_loss: 6.2694\n",
            "\n",
            "Epoch 00026: loss improved from 0.07627 to 0.07590, saving model to Weights-LSTM-improvement-026-0.07590-bigger.hdf5\n",
            "Epoch 27/30\n",
            "138878/138878 [==============================] - 187s 1ms/step - loss: 0.0673 - val_loss: 6.1307\n",
            "\n",
            "Epoch 00027: loss improved from 0.07590 to 0.06726, saving model to Weights-LSTM-improvement-027-0.06726-bigger.hdf5\n",
            "Epoch 28/30\n",
            "138878/138878 [==============================] - 187s 1ms/step - loss: 0.0700 - val_loss: 6.1695\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.06726\n",
            "Epoch 29/30\n",
            "138878/138878 [==============================] - 187s 1ms/step - loss: 0.0713 - val_loss: 6.2134\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.06726\n",
            "Epoch 30/30\n",
            "138878/138878 [==============================] - 187s 1ms/step - loss: 0.0665 - val_loss: 6.2796\n",
            "\n",
            "Epoch 00030: loss improved from 0.06726 to 0.06650, saving model to Weights-LSTM-improvement-030-0.06650-bigger.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f75e80620f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AXMqjkkXi7A"
      },
      "source": [
        "We can see that some files have been downloaded, we can use such files to load the trained weights to be used in untrained models (i.e we don't have to train a model every time we want to use it) \n",
        "\n",
        "### How to Load the Weights ?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sud9lOmYguG"
      },
      "source": [
        "# Load wights file :\n",
        "wights_file = './models/Weights-LSTM-improvement-004-2.49538-bigger.hdf5' # weights file path\n",
        "model.load_weights(wights_file)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSl6u6oWWFyF"
      },
      "source": [
        "Now , after we trained the model ,we can use it to generate fake Taylor Swift lyrics \n",
        "\n",
        "## Generating lyrics \n",
        "We first pick a random seed , then we will use it to generate lyrics character by character ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5EngGk8YuJv",
        "outputId": "05d07c6a-c08a-4b16-b3f9-65207a89b43b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "# set a random seed :\n",
        "start = np.random.randint(0, len(data_X)-1)\n",
        "pattern = data_X[start]\n",
        "print('Seed : ')\n",
        "print(\"\\\"\",''.join([int_chars[value] for value in pattern]), \"\\\"\\n\")\n",
        "\n",
        "# How many characters you want to generate\n",
        "generated_characters = 300\n",
        "\n",
        "# Generate Charachters :\n",
        "for i in range(generated_characters):\n",
        "    x = np.reshape(pattern, ( 1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x,verbose = 0)\n",
        "    index = np.argmax(prediction)\n",
        "    result = int_chars[index]\n",
        "    #seq_in = [int_chars[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print('\\nDone')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed : \n",
            "\"  once, i've been waiting, waiting\n",
            "ooh whoa, ooh whoa\n",
            "and all at once, you are the one, i have been w \"\n",
            "\n",
            "eu h mool shoea\n",
            "a eir, bo ly lean on the sast\n",
            "is tigm's the noen uo doy, fo shey stant tas you fot you srart aoo't you tein so my liost\n",
            "i spaye \n",
            "somethppel' cua\n",
            "iy yas tn mu, io' me\n",
            "ohehip in the uorlirs tiines ho a ban't teit dven aester, tee tame\n",
            "mnweiny you'd be pe k bet thing\n",
            "oe eowt the light i\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7x2UZ09Z_R5"
      },
      "source": [
        "You might noticed that the generated lyrics are not real , and there are many spelling mistakes.\n",
        "\n",
        "You can tweak some parameters and add a Dropout layer to avoid overfitting ,then the model could be better at generating tolerable lyrics.\n",
        "\n",
        "but if you are lazy and don't want to bother yourself trying these steps , try using [textgenrnn](https://goo.gl/E7szXj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nigwY7cRbvoo"
      },
      "source": [
        "## Second way : Using [textgenrnn](https://goo.gl/E7szXj)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j00J7LUckAN"
      },
      "source": [
        "### Import the dependencies :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15yRfdyNcPCc"
      },
      "source": [
        "!pip install -q textgenrnn\n",
        "from google.colab import files\n",
        "from textgenrnn import textgenrnn\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PL6vHmscxRV"
      },
      "source": [
        "### Configure the model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwLJ4CrfcbNn"
      },
      "source": [
        "model_cfg = {\n",
        "    'rnn_size': 500,\n",
        "    'rnn_layers': 12,\n",
        "    'rnn_bidirectional': True,\n",
        "    'max_length': 15,\n",
        "    'max_words': 10000,\n",
        "    'dim_embeddings': 100,\n",
        "    'word_level': False,\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    'line_delimited': True,\n",
        "    'num_epochs': 100,\n",
        "    'gen_epochs': 25,\n",
        "    'batch_size': 750,\n",
        "    'train_size': 0.8,\n",
        "    'dropout': 0.0,\n",
        "    'max_gen_length': 300,\n",
        "    'validation': True,\n",
        "    'is_csv': False\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR8dJVEWc9D5"
      },
      "source": [
        "### Upload the dataset :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wNR9aZYchqr"
      },
      "source": [
        "uploaded = files.upload()\n",
        "all_files = [(name, os.path.getmtime(name)) for name in os.listdir()]\n",
        "latest_file = sorted(all_files, key=lambda x: -x[1])[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD7mluevdBvl"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xuCgouMdIYW"
      },
      "source": [
        "model_name = '500nds_12Lrs_100epchs_Model'\n",
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "train_function(\n",
        "    file_path=latest_file,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=train_cfg['batch_size'],\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    max_gen_length=train_cfg['max_gen_length'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=model_cfg['dim_embeddings'],\n",
        "    word_level=model_cfg['word_level'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKgW9B1Ddcuq"
      },
      "source": [
        "print(textgen.model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGP6jPhYdegM"
      },
      "source": [
        "### Download the trained weights :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY-G1pbgdfDS"
      },
      "source": [
        "files.download('{}_weights.hdf5'.format(model_name))\n",
        "files.download('{}_vocab.json'.format(model_name))\n",
        "files.download('{}_config.json'.format(model_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjU4fA34eVQT"
      },
      "source": [
        "### Load trained model and use it :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm3ZYCanecNP"
      },
      "source": [
        "textgen = textgenrnn(weights_path='6layers30EpochsModel_weights.hdf5',\n",
        "                       vocab_path='6layers30EpochsModel_vocab.json',\n",
        "                       config_path='6layers30EpochsModel_config.json')\n",
        "\n",
        "generated_characters = 300\n",
        "\n",
        "textgen.generate_samples(300)\n",
        "textgen.generate_to_file('lyrics.txt', 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn_HpyWrnFws"
      },
      "source": [
        "Some lyrics generated by a model created using   [textgenrnn](https://goo.gl/E7szXj) :\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "i ' m not your friends\n",
        "and it rains when you ' re not speaking\n",
        "but you think tim mcgraw\n",
        "and i ' m pacing down\n",
        "i ' m comfortable\n",
        "i ' m not a storm in mind\n",
        "you ' re not speaking\n",
        "and i ' m not a saint and i ' m standin ' t know you ' re\n",
        "i ' m wonderstruck\n",
        "and you ' re gay\n",
        "\n",
        "i ' ve been giving out\n",
        "but i ' m just another picture to pay\n",
        "you ' re not asking myself , oh , i ' d go back to december , don ' t know you\n",
        "it ' s killing me like a chalkboard\n",
        "it ' s the one you\n",
        "can ' t you ' re jumping into of you ' re not a last kiss\n",
        "and i ' m just a girl , baby , i ' m alone for me\n",
        "i ' m not a little troubling\n",
        "\n",
        "won ' t you think about a . steps , you roll the stars mind\n",
        "you ' s killing me ? )\n",
        "and i ' m say i won ' t stay beautiful at onto the first page\n",
        "you ' s 2 : pretty\n",
        "and you said real\n",
        "?\n",
        "change makes and oh , who baby , oh , and you talk away\n",
        "and you ' s all a minute , ghosts your arms page\n",
        "these senior making me tough , so hello growing up , we were liar , no one someone perfect day when i came\n",
        "' re not sorry\n",
        "you ' re an innocent\n",
        "on the outskirts\n",
        "\n",
        "ight , don ' t say a house and he ' round\n",
        "she ' re thinking to december all that baby , all everything now\n",
        "and let me when you oh , what to come back my dress\n",
        "always\n",
        "i close both young before\n",
        "at ?\n",
        "yeah\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWBl_cOygt96"
      },
      "source": [
        "We saw how easy and convenient it was using  [textgenrnn](https://goo.gl/E7szXj) , yes the lyrics still not realistic, but there are much less spelling mistakes than the model that we built from scratch.\n",
        "\n",
        "another good thing about  [textgenrnn](https://goo.gl/E7szXj) is that one don't have to deal with any dataset processing, just upload the text dataset and set down with a cup of coffee watching your model training and getting better "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgFMQoJ2ikEG"
      },
      "source": [
        "## Next Steps :\n",
        "Now, after you learned how to make a LSTM RNN from scratch to generate texts , and also how to use Pyhton modules such as [textgenrnn](https://goo.gl/E7szXj) you can do many things using this knowledge :\n",
        "* Try to use other datasets (wikipedia articles , William Shakespeare novevls, etc) to generate novels or articles.\n",
        "* Use  LSTM RNN in other applications than text generating .\n",
        "* Read more about LSTM RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV3bbXu2quZC"
      },
      "source": [
        "## References :\n",
        "* [Text Generation With LSTM Recurrent Neural Networks in Python with Keras](https://goo.gl/Kbpk8S)\n",
        "* [Applied Introduction to LSTMs with GPU for text generation](https://goo.gl/xnQSJU)\n",
        "* [Generating Text Using LSTM RNN](https://goo.gl/7GxTxu)\n",
        "* [textgenrnn](https://goo.gl/E7szXj)\n",
        "* [Train a Text-Generating Neural Network for Free with textgenrnn](https://goo.gl/biaFCr)\n",
        "* [Understanding LSTM Networks](https://goo.gl/dgmxQm)\n",
        "* [Long short-term memory](https://en.wikipedia.org/wiki/Long_short-term_memory)"
      ]
    }
  ]
}